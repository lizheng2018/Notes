# 关于随机森林与集成学习方法 Ensemble Learning

#### 引入结论

* Bagging + 决策树 = 随机森林

* AdaBoost + 决策树 = 提升树

* Gradient Boosting + 决策树 = GBDT

随机森林就是通过**集成学习**的思想将多棵**决策树**集成的一种算法，多棵树一起构成了 ”**森林**“，基本单元是决策树，而它的本质属于机器学习的一大分支——集成学习（Ensemble Learning）方法。

### 集成学习方法

#### Bagging方法

* 从原始样本集中抽取训练集。每轮从原始样本集中使用Bootstrap的方法抽取n个训练样本，**Bootstrap有放回全抽样**（在训练集中，有些样本可能被多次抽取到，而有些样本可能一次都没有被抽中，**而抽样样本大小和原样本大小一样**）。共进行k轮抽取，得到k个训练集。（k个训练集之间是相互独立的）

* 每次使用一个训练集得到一个模型，k个训练集共得到k个模型。（注：这里并没有具体的分类算法或回归方法，我们可以根据具体问题采用不同的分类或回归方法，如决策树、感知器等）

* 对分类问题：将上步得到的k个模型采用投票的方式得到分类结果；对回归问题，计算上述模型的均值作为最后的结果。（所有模型的重要性相同）


#### Boosting方法
简要原理：

将多个弱学习器（树）组装成一个强学习器。在PAC（概率近似正确）学习框架下，则一定可以将弱学习器组装成一个强学习器。俗话说，“三个臭皮匠，顶个诸葛亮”，大概就是这种。

注意：学习器和分类器在描述集成学习方法时，是一样的，叫法不同而已。

关于Boosting的两个核心问题：

* 通过**提高那些在前一轮被弱学习器预测错误样例的权值，减小前一轮预测正确样例的权值**，来使得学习器对误分的数据有较好的效果。

* 通过**加法模型**将弱学习器进行线性组合，比如AdaBoost通过加权多数表决的方式，即增大错误率小的学习器的权值，同时减小错误率较大的学习器的权值。可以理解成专家（错误率小的学习器）具有更多的决议权
     而提升树通过拟合残差的方式逐步减小残差，将每一步生成的模型叠加得到最终模型。

#### 自适应增强算法Adaboost

算法简单原理：

假设拟合第一个学习器，以最大程度保证准确率，或最小程度上减少数量，接着拟合第二个学习器，第二个学习器需要修正第一个学习器的错误，所以我们需要筛选出错误，放大这些错误的权重，使错误的点新的权重相加与正确的点权重相加的和相等，那么这个学习器就着重修正这些错误的点了，以此循环，第三个学习器继续修正第二个学习器的错误，建立在放大第二个学习器判断错误点的权重上，同样使错误的点新的权重相加与正确的点权重相加的和相等，直到建立够我们所需的弱学习器数量，接着，设置这些弱学习器的权重通过weight = ln (correct/incorrect)来求得，最后进行弱学习器的融合，通过上面弱学习的权重来融合，以此来结合强学习器。

#### Bagging、Boosting比较
1. 样本选择上：
     Bagging：训练集是在原始集中有放回选取的，从原始集中选出的各轮训练集之间是独立的
     Boosting：每一轮的训练集不变，只是训练集中每个样例在学习器中的权重发生变化，而权值是根据上一轮的分类结果进行调整
2. 样例权重：
     Bagging：使用均匀取样，每个样例的权重相等
     Boosting：根据错误率不断调整样例的权值，错误率越大则权重越大
3. 预测函数：
     Bagging：所有预测函数的权重相等
     Boosting：每个弱分类器都有相应的权重，对于分类误差小的分类器会有更大的权重
4. 并行计算：
     Bagging：各个预测函数可以并行生成
     Boosting：各个预测函数只能顺序生成，因为后一个模型参数需要前一轮模型的结果

这两种方法都是把若干个学习器整合为一个学习器的方法，只是整合的方式不一样，最终得到不一样的效果，将不同的分类算法套入到此类算法框架中一定程度上会提高了原单一分类器的分类效果，但是也增大了计算量。

### 随机森林Random Forest

Bagging + 决策树，本质是一种bagging的方式

* 从样本集中用Bootstrap采样选出n个样本
* 在树的每个节点上，从所有属性中随机选择k个属性，选择出一个最佳分割属性作为节点
* 重复以上两步m次，比如，建立 m棵CART树
* 这m个CART形成Random Forest

Random Forest可以既可以处理属性为离散值的量，也可以处理属性为连续值的量。
跟bagging的区别是，bagging随机取的样本，Random Forest既随机取了样本，而后又随机取了属性。

#### Random forest的几个特点：

* 具有极好的准确率
* 能够有效地运行在大数据集上
* 能够处理具有高维特征的输入样本，不需要降维
* 可以不用做特征标准化
* 能够评估各个特征的重要性
* 在生成过程中，能够获取到内部生成误差的一种无偏估计
* 对于缺省值问题也能够获得很好得结果
* …

#### 袋外错误率（oob error, out-of-bag error）

随机森林有一个重要的优点就是，没有必要对它进行交叉验证或者用一个独立的测试集来获得误差的一个无偏估计。它可以在内部进行评估，也就是说在生成的过程中就可以对误差建立一个无偏估计。

我们知道，在构建每棵树时，我们对训练集使用了不同的bootstrap sample（随机且有放回地抽取）。所以对于每棵树而言（假设对于第k棵树），大约有1/3的训练实例没有参与第k棵树的生成，它们称为第k棵树的oob样本。

而这样的采样特点就允许我们进行oob估计，它的计算方式如下：

**（note：以样本为单位）**

1. 对每个样本，计算它作为oob样本的树对它的分类情况（约1/3的树）
2. 然后以简单多数投票作为该样本的分类结果
3. 最后用误分个数占样本总数的比率作为随机森林的oob误分率

**oob误分率是随机森林泛化误差的一个无偏估计，它的结果近似于大量计算的k折交叉验证。**



　