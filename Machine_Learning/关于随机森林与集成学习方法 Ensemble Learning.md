# 关于随机森林与集成学习方法 Ensemble Learning

#### 引入结论

* Bagging + 决策树 = 随机森林

* AdaBoost + 决策树 = 提升树

* Gradient Boosting + 决策树 = GBDT

随机森林就是通过**集成学习**的思想将多棵**决策树**集成的一种算法，多棵树一起构成了 ”**森林**“，基本单元是决策树，而它的本质属于机器学习的一大分支——集成学习（Ensemble Learning）方法。

#### Bagging方法

* 从原始样本集中抽取训练集。每轮从原始样本集中使用Bootstrap的方法抽取n个训练样本，**Bootstrap有放回全抽样**（在训练集中，有些样本可能被多次抽取到，而有些样本可能一次都没有被抽中，**而抽样样本大小和原样本大小一样**）。共进行k轮抽取，得到k个训练集。（k个训练集之间是相互独立的）

* 每次使用一个训练集得到一个模型，k个训练集共得到k个模型。（注：这里并没有具体的分类算法或回归方法，我们可以根据具体问题采用不同的分类或回归方法，如决策树、感知器等）

* 对分类问题：将上步得到的k个模型采用投票的方式得到分类结果；对回归问题，计算上述模型的均值作为最后的结果。（所有模型的重要性相同）


#### Boosting方法
简要原理：

将多个弱学习器（树）组装成一个强学习器。在PAC（概率近似正确）学习框架下，则一定可以将弱学习器组装成一个强学习器。俗话说，“三个臭皮匠，顶个诸葛亮”，大概就是这种。

注意：学习器和分类器在描述集成学习方法时，是一样的，叫法不同而已。

关于Boosting的两个核心问题：

* 通过**提高那些在前一轮被弱学习器预测错误样例的权值，减小前一轮预测正确样例的权值**，来使得学习器对误分的数据有较好的效果。

* 通过**加法模型**将弱学习器进行线性组合，比如AdaBoost通过加权多数表决的方式，即增大错误率小的学习器的权值，同时减小错误率较大的学习器的权值。可以理解成专家（错误率小的学习器）具有更多的决议权
     而提升树通过拟合残差的方式逐步减小残差，将每一步生成的模型叠加得到最终模型。

#### 随机森林Random Forest

Bagging + 决策树，本质是一种bagging的方式

* 从样本集中用Bootstrap采样选出n个样本
* 在树的每个节点上，从所有属性中随机选择k个属性，选择出一个最佳分割属性作为节点
* 重复以上两步m次，比如，建立 m棵CART树
* 这m个CART形成Random Forest

Random Forest可以既可以处理属性为离散值的量，也可以处理属性为连续值的量。
跟bagging的区别是，bagging随机取的样本，Random Forest既随机取了样本，而后又随机取了属性。

#### 自适应增强算法Adaboost

算法简单原理：

假设拟合第一个学习器，以最大程度保证准确率，或最小程度上减少数量，接着拟合第二个学习器，第二个学习器需要修正第一个学习器的错误，所以我们需要筛选出错误，放大这些错误的权重，使错误的点新的权重相加与正确的点权重相加的和相等，那么这个学习器就着重修正这些错误的点了，以此循环，第三个学习器继续修正第二个学习器的错误，建立在放大第二个学习器判断错误点的权重上，同样使错误的点新的权重相加与正确的点权重相加的和相等，直到建立够我们所需的弱学习器数量，接着，设置这些弱学习器的权重通过weight = ln (correct/incorrect)来求得，最后进行弱学习器的融合，通过上面弱学习的权重来融合，以此来结合强学习器。

#### Bagging、Boosting比较
1. 样本选择上：
     Bagging：训练集是在原始集中有放回选取的，从原始集中选出的各轮训练集之间是独立的
     Boosting：每一轮的训练集不变，只是训练集中每个样例在学习器中的权重发生变化，而权值是根据上一轮的分类结果进行调整
2. 样例权重：
     Bagging：使用均匀取样，每个样例的权重相等
     Boosting：根据错误率不断调整样例的权值，错误率越大则权重越大
3. 预测函数：
     Bagging：所有预测函数的权重相等
     Boosting：每个弱分类器都有相应的权重，对于分类误差小的分类器会有更大的权重
4. 并行计算：
     Bagging：各个预测函数可以并行生成
     Boosting：各个预测函数只能顺序生成，因为后一个模型参数需要前一轮模型的结果

这两种方法都是把若干个学习器整合为一个学习器的方法，只是整合的方式不一样，最终得到不一样的效果，将不同的分类算法套入到此类算法框架中一定程度上会提高了原单一分类器的分类效果，但是也增大了计算量。

