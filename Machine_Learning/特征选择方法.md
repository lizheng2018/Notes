## 特征选择

几个概念：

方差分析（ANOVA），analysis of variance



### 1. F值和 P-value

scikit-learn中的公式：

第一步：求出ri

![1555052528060](C:\Users\MarioCode\AppData\Roaming\Typora\typora-user-images\1555052528060.png)

Xi是代表所有样本的在i号特征上的取值的n维列向量，分子上其实两个n维列向量的内积，所以ri是一个**数值**，其实就是样本相关系数

第二步：利用ri求出f值

![1555053013370](C:\Users\MarioCode\AppData\Roaming\Typora\typora-user-images\1555053209938.png)

上式才是f_regression中的f值，服从F(1,n−2)分布，f值越大，Xi特征和因变量y之间的相关性就越大

第三步：f_regression还有一个返回值p-value，这个p-value就是用于检验特征与变量之间相关性的，假设你给出α值（常取0.05，0.01），如果你的p-value小于α，那就有把握认为，这个特征和预测变量y之间，具有相关性。比方说你取α=0.05，这就意味着你有95%（也就是1-α）的把握认为，这个特征和预测变量y之间存在相关性

```python
from sklearn.feature_selection import f_regression

X = np.random.rand(1000, 3)
y = X[:, 0] + np.sin(6 * np.pi * X[:, 1]) + 0.1 * np.random.randn(1000)

f_test, p_value= f_regression(X, y)
```

2.Mutual information互信息

```python
mi_importance = mutual_info_regression(X, Y)
feature_name = X.columns
mi_fea_imp = pd.DataFrame({'feature_name':feature_name,'importance':mi_importance} )
mi_fea_imp.sort_values(by='importance', ascending=False, inplace=True)
mi_fea_imp.to_csv('result/mi_importance.csv')
```

![img](https://img-blog.csdn.net/20170424170721945?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvWWFwaGF0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

3. 对称不确定性（SU）

取值在（0，1）之间，值越大，X，Y之间相关性越大，当取值为0，表示X,Y之间相互独立，反之，代表之间具有强依赖性，意味着当知道其中一个变量就可以推测出另一个变量.

![img](https://img-blog.csdn.net/20170424113143229?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvWWFwaGF0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

可以分为C-相关和F-相关，任何一个特征f和类别C之间的关系叫做C-相关，和任何其他特征之间的关系叫做F-相关


