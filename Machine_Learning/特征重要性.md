

## 特征重要性

特征 **j** 的全局重要度通过特征 **j** 在单颗树中的重要度的平均值来衡量： 

![1554630378300](C:\Users\MarioCode\AppData\Roaming\Typora\typora-user-images\1554630378300.png)

其中，M是树的数量。特征 **j** 在单颗树中的重要度的如下：

![1554630408811](C:\Users\MarioCode\AppData\Roaming\Typora\typora-user-images\1554630408811.png)

其中，**L** 为树的叶子节点数量，**L - 1** 即为树的非叶子节点数量（构建的树都是具有左右分支的二叉树），**vt** 是和节点 **t** 相关联的特征，![1554630613297](C:\Users\MarioCode\AppData\Roaming\Typora\typora-user-images\1554630613297.png)是节点**t** 分裂之后平方损失的减少值。

#### scikit-learn代码

**feature_importances_** 属性的计算方法：

```python
def feature_importances_(self):
    total_sum = np.zeros((self.n_features, ), dtype=np.float64)
    for tree in self.estimators_:
        total_sum += tree.feature_importances_ 
    importances = total_sum / len(self.estimators_)
    return importances
```

其中，`self.estimators_`是算法构建出的决策树的数组，`tree.feature_importances_` 是单棵树的特征重要度向量，其计算方法如下：

```cython
cpdef compute_feature_importances(self, normalize=True):
    """Computes the importance of each feature (aka variable)."""
    while node != end_node:
        if node.left_child != _TREE_LEAF:
            # ... and node.right_child != _TREE_LEAF:
            left = &nodes[node.left_child]
            right = &nodes[node.right_child]
            importance_data[node.feature] += (
                node.weighted_n_node_samples * node.impurity -
                left.weighted_n_node_samples * left.impurity -
                right.weighted_n_node_samples * right.impurity)
        node += 1
    importances /= nodes[0].weighted_n_node_samples
    return importances
```

计算所有的非叶子节点在分裂时加权不纯度的减少，减少得越多说明特征越重要。不纯度的减少量，也即纯度的增加量。

不纯度此时包含信息增益或者基尼指数的收益

不纯度的减少实际上就是该节点此次分裂的的收益，因此我们也可以这样理解，节点分裂时收益越大，该节点对应的特征的重要度越高。



1. Correlation coefficient相关系数

![1554868753595](C:\Users\MarioCode\AppData\Roaming\Typora\typora-user-images\1554868753595.png)

where Cov(a,b) is the covariance of a and b, and Var (.) is the variance

2. Pearson correlation coefficient皮尔逊相关系数

![1554869420884](C:\Users\MarioCode\AppData\Roaming\Typora\typora-user-images\1554869420884.png)



3. Mutual information互信息

    ![1554868917602](C:\Users\MarioCode\AppData\Roaming\Typora\typora-user-images\1554868917602.png)

    where p(.) is the probability density function

4. Symmetric uncertainty (SU)

    ![1554868954825](C:\Users\MarioCode\AppData\Roaming\Typora\typora-user-images\1554868954825.png)

    where H(.) is the entropy of a feature

5. Information distance

    ![1554868993513](C:\Users\MarioCode\AppData\Roaming\Typora\typora-user-images\1554868993513.png)

    where H(a|b) is the conditional entropy of a given b

6. Euclidean distance

    ![1554869049617](C:\Users\MarioCode\AppData\Roaming\Typora\typora-user-images\1554869049617.png)



Relevance and redundancy analysis

four groups: 

* (a) completely irrelevant and noisy features
* (b) weakly relevant and redundant features
* (c) weakly relevant and non-redundant features
* (d) strongly relevant features

MRMR (Max-Relevance and Min- Redundancy)



clustering-based feature selections

![1554879270622](C:\Users\MarioCode\AppData\Roaming\Typora\typora-user-images\1554879270622.png)

The clustering-based method might select an irrelevant feature， since the irrelevant features are often clustered together. Therefore, it is better to delete the irrelevant features before feature clustering.



 







Five rank feature selection methods

* ReliefF
* Information Gain (InfoGain)
* mRMR
* JMI
* SVM-RFE

![1554880366192](C:\Users\MarioCode\AppData\Roaming\Typora\typora-user-images\1554880366192.png)

Ensemble feature selection

Random Forest

random subspace method (RSM)



Feature selection and deep learning Deep

Outline of a wrapper method

![1554947794945](C:\Users\MarioCode\AppData\Roaming\Typora\typora-user-images\1554947794945.png)

Outline of a filter method

![1554947809581](C:\Users\MarioCode\AppData\Roaming\Typora\typora-user-images\1554947809581.png)





![1554950238638](C:\Users\MarioCode\AppData\Roaming\Typora\typora-user-images\1554950238638.png)

Supervised feature selection is often oriented to classification
problem, and uses the relevance or correlation between the feature and the class label as its fundamental principle.