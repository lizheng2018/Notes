## 特征重要性

特征 **j** 的全局重要度通过特征 **j** 在单颗树中的重要度的平均值来衡量： 

![1554630378300](C:\Users\MarioCode\AppData\Roaming\Typora\typora-user-images\1554630378300.png)

其中，M是树的数量。特征 **j** 在单颗树中的重要度的如下：

![1554630408811](C:\Users\MarioCode\AppData\Roaming\Typora\typora-user-images\1554630408811.png)

其中，**L** 为树的叶子节点数量，**L - 1** 即为树的非叶子节点数量（构建的树都是具有左右分支的二叉树），**vt** 是和节点 **t** 相关联的特征，![1554630613297](C:\Users\MarioCode\AppData\Roaming\Typora\typora-user-images\1554630613297.png)是节点**t** 分裂之后平方损失的减少值。

#### scikit-learn代码

GradientBoostingClassifier：feature_importances属性的计算方法：

```python
def feature_importances_(self):
    total_sum = np.zeros((self.n_features, ), dtype=np.float64)
    for tree in self.estimators_:
        total_sum += tree.feature_importances_ 
    importances = total_sum / len(self.estimators_)
    return importances
```

其中，self.estimators_是算法构建出的决策树的数组，tree.feature_importances_ 是单棵树的特征重要度向量，其计算方法如下：

```python
cpdef compute_feature_importances(self, normalize=True):
    """Computes the importance of each feature (aka variable)."""
    while node != end_node:
        if node.left_child != _TREE_LEAF:
            # ... and node.right_child != _TREE_LEAF:
            left = &nodes[node.left_child]
            right = &nodes[node.right_child]
            importance_data[node.feature] += (
                node.weighted_n_node_samples * node.impurity -
                left.weighted_n_node_samples * left.impurity -
                right.weighted_n_node_samples * right.impurity)
        node += 1
    importances /= nodes[0].weighted_n_node_samples
    return importances
```

上面的代码经过了简化，保留了核心思想。计算所有的非叶子节点在分裂时加权不纯度的减少，减少得越多说明特征越重要。

不纯度的减少实际上就是该节点此次分裂的收益，因此我们也可以这样理解，节点分裂时收益越大，该节点对应的特征的重要度越高。

