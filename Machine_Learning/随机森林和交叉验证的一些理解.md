# 关于随机森林与集成学习方法 Ensemble Learning

#### 引入结论

* Bagging + 决策树 = 随机森林

* AdaBoost + 决策树 = 提升树

* Gradient Boosting + 决策树 = GBDT

随机森林就是通过==集成学习==的思想将多棵==决策树==集成的一种算法，多棵树一起构成了 ”==森林==“，基本单元是决策树，而它的本质属于机器学习的一大分支——集成学习（Ensemble Learning）方法。

#### Bagging方法

* 从原始样本集中抽取训练集。每轮从原始样本集中使用Bootstrap的方法抽取n个训练样本，==Bootstrap有放回全抽样==（在训练集中，有些样本可能被多次抽取到，而有些样本可能一次都没有被抽中，**而抽样样本大小和原样本大小一样**）。共进行k轮抽取，得到k个训练集。（k个训练集之间是相互独立的）

* 每次使用一个训练集得到一个模型，k个训练集共得到k个模型。（注：这里并没有具体的分类算法或回归方法，我们可以根据具体问题采用不同的分类或回归方法，如决策树、感知器等）

* 对分类问题：将上步得到的k个模型采用投票的方式得到分类结果；对回归问题，计算上述模型的均值作为最后的结果。（所有模型的重要性相同）

    ​     

#### 随机森林Random Forest

Bagging + 决策树，本质是一种bagging的方式

* 从样本集中用Bootstrap采样选出n个样本
* 在树的每个节点上，从所有属性中随机选择k个属性，选择出一个最佳分割属性作为节点
* 重复以上两步m次，比如，建立 m棵CART树
* 这m个CART形成Random Forest

Random Forest可以既可以处理属性为离散值的量，也可以处理属性为连续值的量。
跟bagging的区别是，bagging随机取的样本，Random Forest既随机取了样本，而后又随机取了属性。

#### Boosting方法
简要原理：

将多个弱分类器（树）组装成一个强分类器。在PAC（概率近似正确）学习框架下，则一定可以将弱分类器组装成一个强分类器。俗话说，“三个臭皮匠，顶个诸葛亮”，大概就是这种。

关于Boosting的两个核心问题：

* 通过==提高那些在前一轮被弱分类器预测错误样例的权值，减小前一轮预测正确样例的权值==，来使得分类器对误分的数据有较好的效果。
* 通过==加法模型==将弱分类器进行线性组合，比如AdaBoost通过加权多数表决的方式，即增大错误率小的分类器的权值，同时减小错误率较大的分类器的权值。
     而提升树通过拟合残差的方式逐步减小残差，将每一步生成的模型叠加得到最终模型。

